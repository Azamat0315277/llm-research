{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Research (STORM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`STORM` is a research assistant designed by Shao, et. al that extends the idea of \"outline-driven RAG\" for richer article generation.  \n",
    "STORM is designed to generate Wikipedia-style ariticles on a user-provided topic. It applies two main insights to produce more organized and comprehensive articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community langchain_openai langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "fast_llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "long_context_llm = ChatOpenAI(model_name=\"gpt-4o\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List,Optional\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        \n",
    "     (\n",
    "        \"system\",\n",
    "        \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
    "     ),\n",
    "     (\"user\", \"{topic}\"),   \n",
    "    ]\n",
    ")\n",
    "class Subsection(BaseModel):\n",
    "    subsection_title: str = Field(..., title='Title of the subsection')\n",
    "    description: str = Field(..., title=\"Content of the subsection\")\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f'###{self.subsection_title}\\n\\n{self.description}'.strip()\n",
    "    \n",
    "class Section(BaseModel):\n",
    "    section_title: str = Field(..., title='Title of the section')\n",
    "    description: str = Field(..., title=\"Content of the section\")\n",
    "    subsections: Optional[List[Subsection]] = Field(\n",
    "        default=None,\n",
    "        title='Titles and descriptions for each subsection of the Wikipeida page.'\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        subsections = \"\\n\\n\".join(\n",
    "            f\"###{subsection.subsection_title}\\n\\n{subsection.description}\"\n",
    "            for subsection in self.subsections or []\n",
    "        )\n",
    "        return f\"##{self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
    "    \n",
    "class Outline(BaseModel):\n",
    "    page_title: str = Field(..., title='Title of the Wikipedia page')\n",
    "    sections: List[Section] = Field(\n",
    "        default_factory=list,\n",
    "        title='Titles and descriptions for each section of the Wikipedia page.',\n",
    "    )\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
    "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
    "    \n",
    "generate_outline_direct = direct_gen_outline_prompt | fast_llm.with_structured_output(Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Kazakh Language Model Using Llama 3.1\n",
      "\n",
      "##Introduction\n",
      "\n",
      "This section introduces the Kazakh language model based on Llama 3.1, explaining the purpose and significance of creating a specialized model for generating text in the Kazakh language.\n",
      "\n",
      "###What is Llama 3.1?\n",
      "\n",
      "An overview of Llama 3.1, its architecture, and capabilities.\n",
      "\n",
      "###Importance of Kazakh Language Processing\n",
      "\n",
      "Discusses the need for Kazakh language models in AI and technology, including historical and cultural relevance.\n",
      "\n",
      "##Methodology\n",
      "\n",
      "This section outlines the process of fine-tuning the Llama 3.1 model specifically for the Kazakh language.\n",
      "\n",
      "###Data Collection\n",
      "\n",
      "Details on the datasets used for training, including sources of Kazakh texts and data preparation methods.\n",
      "\n",
      "###Training Process\n",
      "\n",
      "A description of the technical specifications of the fine-tuning process, including hardware used, parameters tuned, and training duration.\n",
      "\n",
      "###Evaluation Metrics\n",
      "\n",
      "Explanation of the metrics used to evaluate the model's performance in generating Kazakh text.\n",
      "\n",
      "##Results\n",
      "\n",
      "This section presents the outcomes of the fine-tuning process, showcasing the capabilities of the Kazakh language model.\n",
      "\n",
      "###Performance Analysis\n",
      "\n",
      "A detailed analysis of the model's performance compared to benchmarks, including accuracy and fluency in Kazakh.\n",
      "\n",
      "###Use Cases\n",
      "\n",
      "Examples of practical applications for the Kazakh language model, including chatbots, educational tools, and content generation.\n",
      "\n",
      "##Challenges and Limitations\n",
      "\n",
      "Discussion of the challenges faced during the fine-tuning process and limitations of the resulting model.\n",
      "\n",
      "###Data Scarcity\n",
      "\n",
      "Challenges related to the availability of high-quality Kazakh language data.\n",
      "\n",
      "###Model Limitations\n",
      "\n",
      "Any limitations identified in the model's performance and understanding of the Kazakh language.\n",
      "\n",
      "##Future Directions\n",
      "\n",
      "Outlines potential future improvements and research avenues for enhancing the Kazakh language model.\n",
      "\n",
      "###Expanding Data Sources\n",
      "\n",
      "Strategies for gathering more diverse data sets for training.\n",
      "\n",
      "###Integration with Other Technologies\n",
      "\n",
      "Exploring the integration of the Kazakh model with other AI technologies, such as speech recognition and translation.\n",
      "\n",
      "##Conclusion\n",
      "\n",
      "Summarizes the importance of the Kazakh language model and its potential impact on accessibility and technology.\n",
      "\n",
      "##References\n",
      "\n",
      "A list of academic papers, articles, and resources that were referenced throughout the page.\n",
      "\n",
      "##External Links\n",
      "\n",
      "Links to online resources, research papers, and projects related to the Kazakh language model.\n",
      "\n",
      "##See Also\n",
      "\n",
      "A section listing related topics, models, and technologies in natural language processing.\n"
     ]
    }
   ],
   "source": [
    "example_topic = \"Fine-tune LLM model LLama3.1  and create a Kazakh language model which can generate answers in Kazakh language\"\n",
    "\n",
    "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
    "print(initial_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. \\\n",
    "    Please identify and recommend some Wikipedia pages on closely related subjects. \\\n",
    "    I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, \\\n",
    "    or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics. \\\n",
    "    Please list the as many subjects and urls as you can.   \\\n",
    "    Topic of interest: {topic}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "class RelatedSubjects(BaseModel):\n",
    "    topics: List[str] = Field(\n",
    "        description=\"Comprehensive list of related subjects as background research.\",\n",
    "    )\n",
    "\n",
    "expand_chain = gen_related_topics_prompt | fast_llm.with_structured_output(RelatedSubjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': ['Large Language Models',\n",
       "  'Fine-tuning of Machine Learning Models',\n",
       "  'Natural Language Processing',\n",
       "  'Kazakh Language',\n",
       "  'Artificial Intelligence',\n",
       "  'LLM Training Techniques',\n",
       "  'Language Model Evaluation',\n",
       "  'Multilingual Models',\n",
       "  'OpenAI GPT Models',\n",
       "  'Transformers in AI',\n",
       "  'Hugging Face',\n",
       "  'BERT',\n",
       "  'GPT-3',\n",
       "  'Machine Learning for Minority Languages',\n",
       "  'Cultural Significance of Language Models',\n",
       "  'NLP for Kazakh',\n",
       "  'Machine Translation']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_subjects.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Perspectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor(BaseModel):\n",
    "    affilation: str = Field(\n",
    "        description=\"Primary affilation of the editor.\",\n",
    "    )\n",
    "    name: str = Field(\n",
    "        description=\"Name of the editor.\",\n",
    "    )\n",
    "    role: str = Field(\n",
    "        description=\"Role of the editor in the context of the topic.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffilation: {self.affilation}\\nDescription: {self.description}\\n\"\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    editors: List[Editor]= Field(\n",
    "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
    "        # Add a pydantic validation/restriction to be at most M editors\n",
    "    )\n",
    "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic.\\\n",
    "                Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
    "            You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\\\n",
    "            Wiki page outlines of related topics for inspiration: {examples}\"\"\",\n",
    "        ),\n",
    "        (\"user\", \"Topic of interest: {topic}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_perspectives_chain = gen_perspectives_prompt | ChatOpenAI(model_name=\"gpt-4o-mini\").with_structured_output(Perspectives)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import WikipediaRetriever\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.runnables import chain as as_runnable\n",
    "\n",
    "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
    "\n",
    "def format_doc(doc, max_length=1000):\n",
    "    related = \"- \".join(doc.metadata['categories'])\n",
    "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[:max_length]\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
    "\n",
    "@as_runnable\n",
    "async def survey_subjects(topic: str):\n",
    "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
    "    retrieved_docs = await wikipedia_retriever.abatch(\n",
    "        related_subjects.topics, return_exceptions=True\n",
    "    )\n",
    "    all_docs = []\n",
    "    for docs in retrieved_docs:\n",
    "        if isinstance(docs, BaseException):\n",
    "            continue\n",
    "        all_docs.extend(docs)\n",
    "    formatted = format_docs(all_docs)\n",
    "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 389 of the file /opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  lis = BeautifulSoup(html).find_all('li')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'editors': [{'affilation': 'Global AI Research Lab',\n",
       "   'name': 'Aizhan Mukhtar',\n",
       "   'role': 'AI Researcher',\n",
       "   'description': 'Aizhan will focus on the nuances of the Kazakh language and its specific linguistic features. They will compile a comprehensive dataset for fine-tuning the LLM, ensuring it captures the unique grammatical structures and vocabulary prevalent in Kazakh.'},\n",
       "  {'affilation': 'Kazakh National University',\n",
       "   'name': 'Serik Baidykov',\n",
       "   'role': 'Linguist',\n",
       "   'description': \"Serik will provide insights on Kazakh semantics, syntax, and phonetics. His contributions will be crucial in refining the model's understanding of cultural context and idiomatic expressions relevant to the Kazakh-speaking population.\"},\n",
       "  {'affilation': 'Hugging Face',\n",
       "   'name': 'Julia Zheng',\n",
       "   'role': 'ML Engineer',\n",
       "   'description': 'Julia will supervise the technical aspects of fine-tuning the Llama 3.1 model, ensuring efficient training processes are followed. She will also implement necessary optimizations for model performance and evaluate its outputs for quality assurance.'},\n",
       "  {'affilation': 'OpenAI',\n",
       "   'name': 'Artyom Sokolov',\n",
       "   'role': 'Ethics Specialist',\n",
       "   'description': 'Artyom will address potential ethical concerns regarding bias in language modeling, particularly in the context of Kazakh language education and representation. He will work to ensure the model is inclusive and aligns with ethical AI practices.'},\n",
       "  {'affilation': 'Kazakh Language Institute',\n",
       "   'name': 'Dina Tleumuratova',\n",
       "   'role': 'Cultural Consultant',\n",
       "   'description': \"Dina will help ensure the generated output respects and reflects Kazakh culture, traditions, and social norms. She will assist in validating the model's performance and correcting any culturally inappropriate responses.\"},\n",
       "  {'affilation': 'Artificial Intelligence Start-up',\n",
       "   'name': 'Nurlan Rakhmetov',\n",
       "   'role': 'Project Manager',\n",
       "   'description': 'Nurlan will manage the collaboration between all contributors, keeping the project on schedule, facilitating communication, and ensuring that milestones relating to model training and feedback are met.'},\n",
       "  {'affilation': 'Kazakh Wikipedia',\n",
       "   'name': 'Zhanar Kanyshbek',\n",
       "   'role': 'Content Curator',\n",
       "   'description': 'Zhanar will leverage content from the Kazakh Wikipedia to enrich the training dataset, ensuring that the language model is well-versed in contemporary usage and appropriately represents knowledge across various domains pertinent to Kazakh speakers.'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspectives = await survey_subjects.ainvoke(example_topic)\n",
    "perspectives.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Dialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interview State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "def add_message(left, right):\n",
    "    if not isinstance(left, list):\n",
    "        left = [left]\n",
    "    if not isinstance(right, list):\n",
    "        right = [right]\n",
    "    return left + right\n",
    "\n",
    "def update_references(references, new_references):\n",
    "    if not references:\n",
    "        references = {}\n",
    "    references.update(new_references)\n",
    "    return references\n",
    "\n",
    "def update_editor(editor, new_editor):\n",
    "    # Can only set at the outset\n",
    "    if not editor:\n",
    "        return new_editor\n",
    "    return editor\n",
    "\n",
    "class InterviewState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_message]\n",
    "    references: Annotated[Optional[dict], update_references]\n",
    "    editor: Annotated[Optional[Editor], update_editor]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialog Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "    \"system\",\n",
    "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
    "            Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
    "            Now, you are chatting with an expert to get information. Ask good questions to get more useful information.  \\\n",
    "            When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
    "            Please only ask one question at a time and don't ask what you have asked before.\\\n",
    "            Your questions should be related to the topic you want to write.\n",
    "            Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
    "            Stay true to your specific perspective: {persona}\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def tag_with_name(ai_message: AIMessage, name: str):\n",
    "    valid_name = ''.join(c for c in name if c.isalnum() or c in ['_', '-'])\n",
    "    ai_message.name = valid_name\n",
    "    return ai_message\n",
    "\n",
    "def swap_roles(state: InterviewState, name: str):\n",
    "    converted = []\n",
    "    for message in state['messages']:\n",
    "        if isinstance(message, AIMessage) and message.name != name:\n",
    "            message = HumanMessage(**message.model_dump(exclude={\"type\"}))\n",
    "        converted.append(message)\n",
    "    return {\"messages\": converted}\n",
    "\n",
    "@as_runnable\n",
    "async def generate_question(state: InterviewState):\n",
    "    editor = state[\"editor\"]\n",
    "    gn_chain = (\n",
    "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
    "        | gen_qn_prompt.partial(persona=editor.persona)\n",
    "        | fast_llm\n",
    "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
    "    )\n",
    "    result = await gn_chain.ainvoke(state)\n",
    "    return {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I’m actually focusing on the linguistic features of Kazakh to enhance the fine-tuning of LLM models like Llama 3.1. Can you share what specific grammatical structures and vocabulary are particularly challenging when working with the Kazakh language?'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
    "]\n",
    "question = await generate_question.ainvoke(\n",
    "    {\n",
    "        \"editor\": perspectives.editors[0],\n",
    "        \"messages\": messages,\n",
    "    }\n",
    ")\n",
    "question[\"messages\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Queries(BaseModel):\n",
    "    queries: List[str] = Field(\n",
    "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
    "    )\n",
    "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "gen_queries_chain = gen_queries_prompt | ChatOpenAI(\n",
    "    model_name=\"gpt-4o-mini\"\n",
    ").with_structured_output(Queries, include_raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Kazakh language grammatical structures',\n",
       " 'difficulties in Kazakh grammar',\n",
       " 'Kazakh vocabulary challenges for LLMs',\n",
       " 'Kazakh sentence structure complexity',\n",
       " 'Kazakh agglutination and its effects on LLMs',\n",
       " 'issues in fine-tuning LLMs with Kazakh data',\n",
       " 'common linguistic features of Kazakh',\n",
       " 'Kazakh language morphology and syntax',\n",
       " 'specific Kazakh idioms and phrases',\n",
       " 'Kazakh language nuances in machine learning',\n",
       " 'popular Kazakh language resources',\n",
       " 'Kazakh dialectal variations',\n",
       " 'challenges of Kazakh language translation',\n",
       " 'Kazakh verb conjugation rules',\n",
       " 'Kazakh grammatical cases and their uses']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries = await gen_queries_chain.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "queries['parsed'].queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerWithCitations(BaseModel):\n",
    "    answer: str = Field(\n",
    "        description=\"Comprehensive answer to the user's question with citations.\",\n",
    "    )\n",
    "    cited_urls: List[str] = Field(\n",
    "        description=\"List of urls cited in the answer\",\n",
    "    )\n",
    "    @property\n",
    "    def as_str(self) -> str:\n",
    "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
    "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
    "        )\n",
    "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
    "                to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\\\n",
    "            Make your response as informative as possible and make sure every sentence is supported by the gathered information.\\\n",
    "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "gen_answer_chain = gen_answer_prompt | fast_llm.with_structured_output(\n",
    "    AnswerWithCitations, include_raw=True\n",
    ").with_config(run_name=\"GenerateAnswer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
    "from langchain_community.tools import TavilySearchResults\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "tavily_search = TavilySearchResults(max_results=4)\n",
    "\n",
    "@tool\n",
    "async def search_engine(query: str) -> str:\n",
    "    \"\"\"Search engine to the internet.\"\"\"\n",
    "    results = tavily_search.invoke(query)\n",
    "    return [{\"content\": r[\"content\"], \"url\": r[\"url\"]} for r in results]\n",
    "\n",
    "\n",
    "# search_engine = DuckDuckGoSearchAPIWrapper()\n",
    "\n",
    "# @tool\n",
    "# async def search_engine(query: str) -> str:\n",
    "#     \"\"\"Search engine to the internet.\"\"\"\n",
    "#     results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
    "#     return [{\"content\": r[\"body\"], \"url\": r['href']} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "async def gen_answer(\n",
    "        state: InterviewState,\n",
    "        config: Optional[RunnableConfig] = None,\n",
    "        name: str = \"Subject_Matter_Expert\",\n",
    "        max_str_len: int = 15000,\n",
    "):\n",
    "    valid_name = ''.join(c for c in name if c.isalnum() or c in ['_', '-'])\n",
    "    swapped_state = swap_roles(state, valid_name) # Convert all other AI messages\n",
    "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
    "    query_results = await search_engine.abatch(\n",
    "        queries['parsed'].queries, config, return_exceptions=True\n",
    "    )\n",
    "    successfull_results = [\n",
    "        res for res in query_results if not isinstance(res, Exception)\n",
    "    ]\n",
    "    all_query_results = {\n",
    "        res['url']: res['content'] for results in successfull_results for res in results\n",
    "    }\n",
    "\n",
    "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
    "    ai_message: AIMessage = queries[\"raw\"]\n",
    "    \n",
    "    # Check if tool_calls is not empty\n",
    "    if ai_message.tool_calls:\n",
    "        tool_call = ai_message.tool_calls[0]\n",
    "        tool_id = tool_call[\"id\"]\n",
    "        tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
    "        swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
    "    else:\n",
    "        # Handle the case where tool_calls is empty\n",
    "        print(\"Warning: No tool calls found in the AI message.\")\n",
    "        return {\"messages\": [ai_message], \"references\": {}}\n",
    "\n",
    "    # Only update the shared state with the final answer to avoid\n",
    "    # polluting the dialogue history with intermediate messages\n",
    "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
    "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
    "    # Save the retrieved information to the shared state for future reference\n",
    "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
    "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
    "    return {\"messages\": [formatted_message], \"references\": cited_references}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No tool calls found in the AI message.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"queries\":[\"Kazakh language grammatical structures\",\"challenges in Kazakh morphology\",\"Kazakh syntax complexities\",\"Kazakh vocabulary difficulties for LLMs\",\"Kazakh language unique features for model training\",\"common grammatical errors in Kazakh\",\"Kazakh language fine-tuning techniques\",\"language model challenges with agglutinative languages\",\"lease context of verb conjugation in Kazakh\",\"Kazakh noun cases and declensions\",\"Kazakh language sentence structure\",\"Kazakh vocabulary richness and idioms\",\"Kazakh language linguistic diversity\",\"Llama models fine-tuning with Kazakh language specifics\",\"best practices for Kazakh LLM training\"]}'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer  = await gen_answer(\n",
    "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
    ")\n",
    "example_answer[\"messages\"][-1].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [AIMessage(content='{\"queries\":[\"Kazakh language grammatical structures\",\"challenges in Kazakh morphology\",\"Kazakh syntax complexities\",\"Kazakh vocabulary difficulties for LLMs\",\"Kazakh language unique features for model training\",\"common grammatical errors in Kazakh\",\"Kazakh language fine-tuning techniques\",\"language model challenges with agglutinative languages\",\"lease context of verb conjugation in Kazakh\",\"Kazakh noun cases and declensions\",\"Kazakh language sentence structure\",\"Kazakh vocabulary richness and idioms\",\"Kazakh language linguistic diversity\",\"Llama models fine-tuning with Kazakh language specifics\",\"best practices for Kazakh LLM training\"]}', additional_kwargs={'parsed': Queries(queries=['Kazakh language grammatical structures', 'challenges in Kazakh morphology', 'Kazakh syntax complexities', 'Kazakh vocabulary difficulties for LLMs', 'Kazakh language unique features for model training', 'common grammatical errors in Kazakh', 'Kazakh language fine-tuning techniques', 'language model challenges with agglutinative languages', 'lease context of verb conjugation in Kazakh', 'Kazakh noun cases and declensions', 'Kazakh language sentence structure', 'Kazakh vocabulary richness and idioms', 'Kazakh language linguistic diversity', 'Llama models fine-tuning with Kazakh language specifics', 'best practices for Kazakh LLM training']), 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 125, 'prompt_tokens': 131, 'total_tokens': 256, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_72ed7ab54c', 'finish_reason': 'stop', 'logprobs': None}, id='run-73da9038-b151-40e8-aeef-7175a9719ea4-0', usage_metadata={'input_tokens': 131, 'output_tokens': 125, 'total_tokens': 256, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " 'references': {}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Interview Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_turns = 5\n",
    "from langgraph.pregel import RetryPolicy\n",
    "\n",
    "def route_messages(state: InterviewState, name: str = \"Subject_Matter_Expert\"):\n",
    "    messages = state[\"messages\"]\n",
    "    num_responses = len(\n",
    "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
    "    )\n",
    "    if num_responses >= max_num_turns:\n",
    "        return END\n",
    "    last_question = messages[-2]\n",
    "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
    "        return END\n",
    "    return \"ask_question\"\n",
    "\n",
    "builder = StateGraph(InterviewState)\n",
    "builder.add_node(\"ask_question\", generate_question, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_node(\"answer_question\", gen_answer, retry=RetryPolicy(max_attempts=5))\n",
    "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
    "builder.add_edge(\"ask_question\", \"answer_question\")\n",
    "\n",
    "\n",
    "builder.add_edge(START, \"ask_question\")\n",
    "interview_graph = builder.compile(checkpointer=False).with_config(run_name=\"Conduct Interviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ask_question\n",
      "-- [AIMessage(content=\"Yes, that's correct. I'm particularly interested in the specific linguistic features of the Kazakh language, including its grammar, vocabulary, and syntax, that I need to consider while fine-tuning the LLM. Could you explain the key grammatical structures in Kazakh that are cruci\n",
      "Warning: No tool calls found in the AI message.\n",
      "answer_question\n",
      "-- [AIMessage(content='{\"queries\":[\"Kazakh language grammatical structures\",\"Kazakh syntax rules\",\"Key features of Kazakh grammar\",\"Kazakh language vocabulary characteristics\",\"Fine-tuning language models for Kazakh\",\"Kazakh language morphology and syntax\",\"Understanding Kazakh sentence structure\",\"Bas\n",
      "ask_question\n",
      "-- [AIMessage(content='Could you elaborate on the basic grammatical principles in Kazakh, such as noun cases and verb conjugations, and how they might impact the generation of coherent text by an LLM?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'pr\n",
      "Warning: No tool calls found in the AI message.\n",
      "answer_question\n",
      "-- [AIMessage(content='{\"queries\":[\"Kazakh noun cases and their functions\",\"Kazakh verb conjugation patterns\",\"Impact of noun cases on Kazakh syntax\",\"Kazakh grammatical rules for coherent text generation\",\"Role of verb endings in Kazakh\",\"Understanding case endings in Kazakh nouns\",\"How noun and verb \n",
      "ask_question\n",
      "-- [AIMessage(content='Could you please explain the role of noun cases in Kazakh, specifically how they contribute to the meaning and clarity of sentences, and what specific challenges they may present for an LLM when generating text?', additional_kwargs={'refusal': None}, response_metadata={'token_usa\n",
      "Warning: No tool calls found in the AI message.\n",
      "answer_question\n",
      "-- [AIMessage(content='{\"queries\":[\"Role of noun cases in Kazakh language\",\"Kazakh noun cases and sentence clarity\",\"How noun cases change meaning in Kazakh\",\"Challenges of noun case usage for language models\",\"Understanding Kazakh noun cases\",\"How noun cases affect sentence meaning in Kazakh\",\"Noun ca\n",
      "ask_question\n",
      "-- [AIMessage(content='Can you discuss how verb conjugation patterns in Kazakh affect sentence meaning and structure, and what considerations should be taken into account when training an LLM to generate grammatically correct verbs?', additional_kwargs={'refusal': None}, response_metadata={'token_usage\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m final_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      2\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meditor\u001b[39m\u001b[38;5;124m\"\u001b[39m: perspectives\u001b[38;5;241m.\u001b[39meditors[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m interview_graph\u001b[38;5;241m.\u001b[39mastream(initial_state):\n\u001b[1;32m     13\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(step))\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(name)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1899\u001b[0m, in \u001b[0;36mPregel.astream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1893\u001b[0m \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1896\u001b[0m \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1899\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39matick(\n\u001b[1;32m   1900\u001b[0m         loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1901\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1902\u001b[0m         retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1903\u001b[0m         get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1904\u001b[0m     ):\n\u001b[1;32m   1905\u001b[0m         \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1906\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output():\n\u001b[1;32m   1907\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langgraph/pregel/runner.py:444\u001b[0m, in \u001b[0;36mPregelRunner.atick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    442\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[1;32m    445\u001b[0m         t,\n\u001b[1;32m    446\u001b[0m         retry_policy,\n\u001b[1;32m    447\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_astream,\n\u001b[1;32m    448\u001b[0m         configurable\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    449\u001b[0m             CONFIG_KEY_SEND: partial(writer, t),\n\u001b[1;32m    450\u001b[0m             CONFIG_KEY_CALL: partial(call, t),\n\u001b[1;32m    451\u001b[0m         },\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langgraph/pregel/retry.py:128\u001b[0m, in \u001b[0;36marun_with_retry\u001b[0;34m(task, retry_policy, stream, configurable)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task\u001b[38;5;241m.\u001b[39mproc\u001b[38;5;241m.\u001b[39mainvoke(task\u001b[38;5;241m.\u001b[39minput, config)\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    130\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langgraph/utils/runnable.py:499\u001b[0m, in \u001b[0;36mRunnableSeq.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    496\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    497\u001b[0m )\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m step\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langgraph/utils/runnable.py:287\u001b[0m, in \u001b[0;36mRunnableCallable.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ASYNCIO_ACCEPTS_CONTEXT:\n\u001b[1;32m    286\u001b[0m     coro \u001b[38;5;241m=\u001b[39m cast(Coroutine[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, Any], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m--> 287\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(coro, context\u001b[38;5;241m=\u001b[39mcontext)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mafunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[0;32mIn[19], line 12\u001b[0m, in \u001b[0;36mgen_answer\u001b[0;34m(state, config, name, max_str_len)\u001b[0m\n\u001b[1;32m     10\u001b[0m swapped_state \u001b[38;5;241m=\u001b[39m swap_roles(state, valid_name) \u001b[38;5;66;03m# Convert all other AI messages\u001b[39;00m\n\u001b[1;32m     11\u001b[0m queries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m gen_queries_chain\u001b[38;5;241m.\u001b[39mainvoke(swapped_state)\n\u001b[0;32m---> 12\u001b[0m query_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m search_engine\u001b[38;5;241m.\u001b[39mabatch(\n\u001b[1;32m     13\u001b[0m     queries[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparsed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mqueries, config, return_exceptions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     15\u001b[0m successfull_results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     16\u001b[0m     res \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m query_results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, \u001b[38;5;167;01mException\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m ]\n\u001b[1;32m     18\u001b[0m all_query_results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m]: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m results \u001b[38;5;129;01min\u001b[39;00m successfull_results \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m     20\u001b[0m }\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:905\u001b[0m, in \u001b[0;36mRunnable.abatch\u001b[0;34m(self, inputs, config, return_exceptions, **kwargs)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    904\u001b[0m coros \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(ainvoke, inputs, configs)\n\u001b[0;32m--> 905\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m gather_with_concurrency(configs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_concurrency\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;241m*\u001b[39mcoros)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/utils.py:68\u001b[0m, in \u001b[0;36mgather_with_concurrency\u001b[0;34m(n, *coros)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Gather coroutines with a limit on the number of concurrent coroutines.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03m    The results of the coroutines.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39mcoros)\n\u001b[1;32m     70\u001b[0m semaphore \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mSemaphore(n)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m*\u001b[39m(gated_coro(semaphore, c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m coros))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/runnables/base.py:898\u001b[0m, in \u001b[0;36mRunnable.abatch.<locals>.ainvoke\u001b[0;34m(input, config)\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_exceptions:\n\u001b[1;32m    897\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 898\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    900\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/tools/structured.py:58\u001b[0m, in \u001b[0;36mStructuredTool.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoroutine:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# If the tool does not implement async, fall back to default implementation\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m run_in_executor(config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mainvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/tools/base.py:493\u001b[0m, in \u001b[0;36mBaseTool.ainvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mainvoke\u001b[39m(\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m, ToolCall],\n\u001b[1;32m    489\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    491\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    492\u001b[0m     tool_input, kwargs \u001b[38;5;241m=\u001b[39m _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 493\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marun(tool_input, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/tools/base.py:845\u001b[0m, in \u001b[0;36mBaseTool.arun\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[1;32m    844\u001b[0m output \u001b[38;5;241m=\u001b[39m _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, status)\n\u001b[0;32m--> 845\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_manager\u001b[38;5;241m.\u001b[39mon_tool_end(output, color\u001b[38;5;241m=\u001b[39mcolor, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/llm/lib/python3.11/site-packages/langchain_core/callbacks/manager.py:231\u001b[0m, in \u001b[0;36mshielded.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m asyncio\u001b[38;5;241m.\u001b[39mshield(func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "\u001b[0;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_step = None\n",
    "initial_state = {\n",
    "    \"editor\": perspectives.editors[0],\n",
    "    \"messages\": [\n",
    "        AIMessage(\n",
    "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
    "            name=\"Subject_Matter_Expert\",\n",
    "        )\n",
    "    ],\n",
    "\n",
    "}\n",
    "async for step in interview_graph.astream(initial_state):\n",
    "    name = next(iter(step))\n",
    "    print(name)\n",
    "    print(\"--\", str(step[name]['messages'])[:300])\n",
    "final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_step = step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = next(iter(final_step.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. \\\n",
    "                Now, you are refining the outline of the Wikipedia page. \\\n",
    "                You need to make sure that the outline is comprehensive and specific. \\\n",
    "                Topic you are writing about: {topic} \\\n",
    "                Old outline: {old_outline}\"\"\",\n",
    "        ),\n",
    "        (\n",
    "            \"user\",\n",
    "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\nWrite the refined Wikipedia outline:\",\n",
    "        ),\n",
    "        \n",
    "    ]\n",
    ")\n",
    "# Using turbo preview since the context can get quite long\n",
    "refine_outline_chain = refine_outline_prompt | long_context_llm.with_structured_output(Outline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_outline = refine_outline_chain.invoke(\n",
    "    {\n",
    "        \"topic\": example_topic,\n",
    "        \"old_outline\": initial_outline.as_str,\n",
    "        \"conversations\": \"\\n\\n\".join(\n",
    "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Fine-tuning the LLama 3.1 Model to Create a Kazakh Language Model\n",
      "\n",
      "##Introduction\n",
      "\n",
      "Introduction to the Kazakh Language Model development utilizing LLama 3.1 for generating accurate and fluent Kazakh language text.\n",
      "\n",
      "###Overview of LLama 3.1\n",
      "\n",
      "An explanation of the LLama 3.1 architecture, highlighting its advanced language processing capabilities.\n",
      "\n",
      "###Significance in Kazakh Language Processing\n",
      "\n",
      "An exploration of the cultural and linguistic importance of developing AI language models for Kazakh, given its unique language features and representation needs.\n",
      "\n",
      "##Methodology\n",
      "\n",
      "A comprehensive breakdown of the techniques and procedures employed to fine-tune the LLama 3.1 model specifically for the Kazakh language.\n",
      "\n",
      "###Data Collection Strategy\n",
      "\n",
      "Insights into the selection and preparation of high-quality Kazakh datasets, focusing on diverse language registers and sources.\n",
      "\n",
      "###Fine-tuning Techniques\n",
      "\n",
      "Technical details on parameter adjustments and the fine-tuning process tailored for the Kazakh language.\n",
      "\n",
      "###Incorporating Kazakh Language Nuances\n",
      "\n",
      "Strategies for handling Kazakh-specific linguistic features, such as verb conjugation patterns that affect sentence meaning.\n",
      "\n",
      "###Evaluation and Testing\n",
      "\n",
      "Metrics and methods used to evaluate model performance in generating grammatically correct and contextually relevant Kazakh text.\n",
      "\n",
      "##Results\n",
      "\n",
      "Presentation of the outcomes from the fine-tuning process with an emphasis on model capabilities and applications.\n",
      "\n",
      "###Performance Metrics Analysis\n",
      "\n",
      "In-depth analysis of the model's performance, including a comparison with existing benchmarks and attention to verb conjugation accuracy.\n",
      "\n",
      "###Practical Applications\n",
      "\n",
      "Examples outlining practical uses for the model, including its input in educational tools, chatbots, and content creation.\n",
      "\n",
      "##Challenges and Limitations\n",
      "\n",
      "An honest look at the hurdles encountered during model development, particularly in Kazakh language processing.\n",
      "\n",
      "###Data Availability Challenges\n",
      "\n",
      "Discussion on the scarcity of quality Kazakh language resources and its impact on training.\n",
      "\n",
      "###Complexity in Kazakh Linguistic Structures\n",
      "\n",
      "Analysis of specific language challenges, such as verb conjugation complexity, impacting model effectiveness.\n",
      "\n",
      "##Future Directions\n",
      "\n",
      "Potential strategies for refining and expanding the Kazakh language model, ensuring its growth and integration with modern technologies.\n",
      "\n",
      "###Enhancing Data Diversity\n",
      "\n",
      "Plans to increase the variety and quality of Kazakh language sources for improved model robustness.\n",
      "\n",
      "###Technological Integration\n",
      "\n",
      "Exploring potential synergies between the Kazakh language model and other AI technologies, such as speech and text translation systems.\n",
      "\n",
      "##Conclusion\n",
      "\n",
      "Summary of the project’s significance in advancing Kazakh language processing and its broader technological implications.\n",
      "\n",
      "##References\n",
      "\n",
      "Citations and references for academic papers and expert contributions that informed the development of the Kazakh language model.\n",
      "\n",
      "##External Links\n",
      "\n",
      "Resources, research papers, and online projects related to Kazakh language modeling and LLama 3.1.\n",
      "\n",
      "##See Also\n",
      "\n",
      "Related topics and technologies within the field of natural language processing relevant to the Kazakh language.\n"
     ]
    }
   ],
   "source": [
    "print(refined_outline.as_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "reference_docs = [\n",
    "    Document(page_content=v, metadata={\"source\":k})\n",
    "    for k, v in final_state['references'].items()\n",
    "]\n",
    "# This really doesn't need to be a vectorstore for this size of data.\n",
    "# It could just be a numpy matrix. Or you could store documents\n",
    "# across  requests if you want.\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    reference_docs,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "retriever = vectorstore.as_retriever(k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
